{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1qPeOGVfkb_",
        "outputId": "994e2a4f-b724-4a80-8a27-21dbaa709db8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb  6 08:49:08 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Mounted at /content/drive\n",
            " 1DCNN_trainer.ipynb\t        test_project\n",
            " colab_no_nlp_1DCNN.ipynb       train_badword_KoBert.ipynb\n",
            " exam\t\t\t       'train-kobert-for-wellness (1).ipynb'\n",
            " final_project\t\t        train-kobert-for-wellness.ipynb\n",
            "'GRU_trainer (1).ipynb'         train-kogpt2-for-wellness.ipynb\n",
            "'GRU_trainer (2).ipynb'         Untitled\n",
            "'GRU_trainer (3).ipynb'         Untitled0.ipynb\n",
            " GRU_trainer.ipynb\t       'Untitled (1)'\n",
            "'inference (1).ipynb'\t        Untitled1.ipynb\n",
            " inference.ipynb\t       'Untitled (2)'\n",
            " JJI_Project\t\t       'Untitled (3)'\n",
            " predict_badword_KoBert.ipynb\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 2)) (4.62.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting transformers==4.2.2\n",
            "  Downloading transformers-4.2.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 5)) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (4.10.1)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 53.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 59.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (21.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 5)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2->-r drive/My Drive/Colab Notebooks/JJI_Project/requirements.txt (line 4)) (1.1.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, sentencepiece\n",
            "Successfully installed sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.9.4 transformers-4.2.2\n",
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-uzdr_n1s/kobert-tokenizer_f3a9b58d66424ae49958f75696b15403\n",
            "  Running command git clone -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-uzdr_n1s/kobert-tokenizer_f3a9b58d66424ae49958f75696b15403\n",
            "Building wheels for collected packages: kobert-tokenizer\n",
            "  Building wheel for kobert-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert-tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4649 sha256=c3b31acccdd9ef838cafd1ac697ade7e559db528e13c9238977ef54c025184e3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-txoes9st/wheels/10/b4/d9/cb627bbfaefa266657b0b4e8127f7bf96d27376fa1a23897b4\n",
            "Successfully built kobert-tokenizer\n",
            "Installing collected packages: kobert-tokenizer\n",
            "Successfully installed kobert-tokenizer-0.1\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls drive/'My Drive'/'Colab Notebooks'/\n",
        "!pip install -r drive/'My Drive'/'Colab Notebooks'/JJI_Project/requirements.txt\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "import sys\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "88ApclaNjVvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0adb4ce-8b19-406b-a64c-9555ea76af1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "from JJI_Project.dataloader.dataloader import GPT2Dataset\n",
        "from JJI_Project.model.kogpt2_model import GPT2Chat\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9vwUZQ9j0DN",
        "outputId": "99b56660-76c5-4215-e5ac-40892e9fad23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1292 entries, 0 to 1291\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Q       1292 non-null   object\n",
            " 1   A       1292 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 20.3+ KB\n"
          ]
        }
      ],
      "source": [
        "ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(ctx)\n",
        "\n",
        "root_path='drive/My Drive/Colab Notebooks/JJI_Project'\n",
        "\n",
        "keti_file = open(f'{root_path}/input/KETI_office_dataset_for_autoregressive.txt', 'r', encoding='utf-8')\n",
        "Q = []\n",
        "A = []\n",
        "while True:\n",
        "    line = keti_file.readline()\n",
        "    if not line:\n",
        "        break\n",
        "    datas = line.split(\"    \")\n",
        "    Q.append(datas[0])\n",
        "    A.append(datas[1][:-1])\n",
        "keti_Data = pd.DataFrame({'Q': Q, 'A': A})\n",
        "keti_Data.info()\n",
        "\n",
        "\n",
        "epochs = 1\n",
        "batch_size = 8\n",
        "Sneg = -1e18\n",
        "learning_rate = 3e-5\n",
        "\n",
        "model = GPT2Chat()\n",
        "model.to(device)\n",
        "train_dataset = GPT2Dataset(keti_Data)\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    data = [item[0] for item in batch]\n",
        "    mask = [item[1] for item in batch]\n",
        "    label = [item[2] for item in batch]\n",
        "    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate_batch,)\n",
        "\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path =f\"{root_path}/output\"\n",
        "save_ckpt_path = f'{checkpoint_path}/model/cp_chatbot.pt'\n",
        "\n",
        "pre_epoch, pre_loss, train_step = 0, 0, 0\n",
        "if os.path.isfile(save_ckpt_path):\n",
        "    checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "    \n",
        "    pre_epoch = checkpoint['Epoch']\n",
        "    model.load_state_dict(checkpoint['State_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "    # pre_loss = checkpoint['Loss']\n",
        "    # train_step =  checkpoint['Train_no']\n",
        "    # total_train_step =  checkpoint['Total_train_step']\n",
        "\n",
        "    print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")  #, loss={pre_loss}\\n\")\n",
        "    # best_epoch += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpDOt1ig6J1M",
        "outputId": "9e6c48a4-af15-4aa7-ec4e-2e4a175c7dc8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load pretrain from: drive/My Drive/Colab Notebooks/JJI_Project/output/model/cp_chatbot.pt, epoch=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, (token_ids, mask, label) in enumerate(tqdm(train_dataloader)):\n",
        "        token_ids = token_ids.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(token_ids).logits\n",
        "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
        "        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n",
        "        loss = criterion(mask_out.transpose(2, 1), label)\n",
        "        # 평균 loss 만들기 avg_loss[0] / avg_loss[1] <- loss 정규화\n",
        "        avg_loss = loss.sum() / mask.sum()\n",
        "        avg_loss.backward()\n",
        "        # 학습 끝\n",
        "        optimizer.step()\n",
        "    state = {'Epoch': epoch,\n",
        "             'State_dict': model.state_dict(),\n",
        "             'optimizer': optimizer.state_dict()}\n",
        "    torch.save(state, f'{checkpoint_path}/model/cp_chatbot.pt')\n",
        "    print('epoch:', epoch, 'loss:', avg_loss)\n",
        "\n",
        "model.eval()\n",
        "torch.save(model, f'{checkpoint_path}/model/chatbot.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAcJKPhQ7apd",
        "outputId": "956eb1f0-78ed-4348-fa3d-a24076f2cda9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 162/162 [00:23<00:00,  6.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 loss: tensor(32.7905, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JzY2OujT7bch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nlDDRyEp7bZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 참고 코드\n",
        "\n",
        "# checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "# save_ckpt_path = f\"{checkpoint_path}/test_GPT.pth\"\n",
        "\n",
        "\n",
        "# n_epoch = 5         # Num of Epoch\n",
        "# batch_size = 1      # 배치 사이즈\n",
        "# ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = torch.device(ctx)\n",
        "# save_step = 100 # 학습 저장 주기\n",
        "\n",
        "# learning_rate = 5e-5  # Learning Rate\n",
        "\n",
        "# dataset= WellnessAutoRegressiveDataset(data_path)\n",
        "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# model = DialogKoGPT2()\n",
        "# model.to(device)\n",
        "\n",
        "\n",
        "# loss_fct = torch.nn.CrossEntropyLoss(ignore_index=3)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# ###################\n",
        "# pre_epoch, pre_loss, train_step = 0, 0, 0\n",
        "# if os.path.isfile(save_ckpt_path):\n",
        "#     checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "#     pre_epoch = checkpoint['epoch']\n",
        "#     pre_loss = checkpoint['loss']\n",
        "#     train_step =  checkpoint['train_no']\n",
        "#     total_train_step =  checkpoint['total_train_step']\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "#     print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")  #, loss={pre_loss}\\n\")\n",
        "#     # best_epoch += 1\n",
        "\n",
        "\n",
        "# ###################\n",
        "\n",
        "# losses =[]\n",
        "# for epoch in range(n_epoch):\n",
        "#     count = 0\n",
        "#     with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
        "#         for i, data in enumerate(train_loader):\n",
        "#             optimizer.zero_grad()\n",
        "#             data = torch.stack(data)  # list of Tensor로 구성되어 있기 때문에 list를 stack을 통해 변환해준다.\n",
        "#             data = data.transpose(1, 0)\n",
        "#             data= data.to(ctx)\n",
        "\n",
        "#             outputs = model(data, labels=data)\n",
        "#             _, logits = outputs[:2]\n",
        "\n",
        "#             shift_logits = logits[..., :-1, :].contiguous()\n",
        "#             shift_labels = data[..., 1:].contiguous()\n",
        "\n",
        "#             loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             losses.append(loss.item())\n",
        "\n",
        "\n",
        "#             # if count % 10 == 0:\n",
        "#             #     print('epoch no.{} train no.{}  loss = {}'.format(epoch, count + 1, loss))\n",
        "#             if (count > 0 and count % save_step == 0) or (len(data) < batch_size):\n",
        "#                 torch.save({\n",
        "#                     'epoch': epoch,\n",
        "#                     'train_no': count,\n",
        "#                     'model_state_dict': model.state_dict(),\n",
        "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                     'total_train_step': len(train_loader),\n",
        "#                     'loss': loss\n",
        "#                 }, save_ckpt_path)\n",
        "#             count += 1\n",
        "#             pbar.update(1)\n",
        "#             pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")\n",
        "\n",
        "#             # if count % 1000 == 0:\n",
        "#             #     print('epoch no.{} train no.{}  loss = {}'.format(epoch, count + 1, loss))\n",
        "#             # if (count > 0 and count % save_step == 0) or (len(data) < batch_size):\n",
        "#             #     torch.save({\n",
        "#             #         'epoch': epoch,\n",
        "#             #         'train_no': count,\n",
        "#             #         'model_state_dict': model.state_dict(),\n",
        "#             #         'total_train_step': len(train_loader),\n",
        "#             #         'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             #         'loss': loss\n",
        "#             #     }, save_ckpt_path)\n",
        "            \n",
        "           \n",
        "\n",
        "# ########################################################\n",
        "# torch.save({\n",
        "#     'epoch': epoch,\n",
        "#     'train_no': count,\n",
        "#     'model_state_dict': model.state_dict(),\n",
        "#     'total_train_step': len(train_loader),\n",
        "#     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#     'loss': loss\n",
        "#     }, save_ckpt_path)\n",
        "# ########################################################"
      ],
      "metadata": {
        "id": "Kyh3Uzscafj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "colab_train_KoGPT2_JJI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}